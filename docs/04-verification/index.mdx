---
sidebar_position: 5
title: "Verification Relevance"
---

import ConceptCheck from '@site/src/components/ConceptCheck';
import Scenario from '@site/src/components/Scenario';
import NetworkVisibilityMap from '@site/src/components/NetworkVisibilityMap';

# Chapter 04: Verification Relevance

> *What can you actually observe? What's invisible? What does this mean for AI governance and safety verification?*

## The Verification Problem

As AI systems become more capable and consequential, governments and institutions are asking a fundamental question: **can we verify what's happening inside AI training clusters?**

This isn't an abstract question. Proposed compute governance frameworks, international agreements on AI safety, and industry self-regulation commitments all depend on some ability to verify claims about how compute is being used. The infrastructure details in the preceding chapters matter because they define what's observable and what isn't.

### Why Infrastructure Knowledge Matters for Policy

Many governance proposals implicitly assume that compute usage is straightforward to monitor. Having read the preceding chapters, you now know this isn't true:

- **NVLink traffic is invisible** to network-based monitoring ([Chapter 02](/docs/interconnects)). As NVLink domains grow, the blind spot grows with them.
- **Power draw reveals activity** but not intent ([Chapter 01](/docs/physical-architecture)). You can tell a facility is running GPUs but not what they're computing.
- **Cluster telemetry requires trust** ([Chapter 03](/docs/cluster-management)). Most metrics are generated by software running on machines controlled by the operator.

A governance framework that doesn't account for these realities will be either ineffective (easy to circumvent) or overly burdensome (requiring impractical levels of access). The goal of this chapter is to help you think precisely about what verification can and can't achieve.

## The Observability Boundary

Not all traffic in an AI cluster is created equal from a monitoring perspective. The critical distinction is between **NVLink traffic** and **network fabric traffic**:

| Property | NVLink Domain | InfiniBand Fabric |
|----------|--------------|-------------------|
| Where it flows | Within an NVLink-connected group (e.g., within an NVL72 rack) | Between racks and across the cluster |
| Bandwidth | 900 GB/s per GPU | 400 Gb/s per link |
| Observable via SmartNIC? | **No** | **Yes** |
| Observable via switch telemetry? | **No** | **Yes** |
| Observable via BMC? | Partially (power, temperature) | N/A |

This boundary is the single most important concept for verification architecture. A SmartNIC or network TAP placed on the InfiniBand fabric can observe inter-rack traffic patterns, but is completely blind to GPU-to-GPU communication within an NVLink domain. For an NVL72 rack, that means 72 GPUs' worth of internal communication is invisible to network-based monitoring.

### Explore the Visibility Boundaries

Use the interactive map below to see exactly what each type of monitoring probe can and cannot observe:

<NetworkVisibilityMap />

The key insight: **no single probe type provides complete visibility**. Every monitoring approach has blind spots. Robust verification requires combining multiple independent observation points and cross-referencing their data for consistency.

<ConceptCheck
  question="A verification system uses SmartNIC-based monitoring on every server's InfiniBand port. A training run is configured to use 72 GPUs within a single NVL72 rack. What fraction of the training run's GPU-to-GPU communication can the verification system observe?"
  options={[
    {label: "(a) 100% — SmartNICs see all traffic on the servers they're attached to"},
    {label: "(b) The fraction that crosses the InfiniBand boundary, which depends on the parallelism strategy"},
    {label: "(c) Essentially none — all 72 GPUs communicate via NVLink within the rack, which SmartNICs can't see", correct: true},
    {label: "(d) About 50% — SmartNICs see inter-node traffic but not intra-node traffic"},
  ]}
  explanation="If the entire training job fits within a single NVL72 rack, all GPU-to-GPU communication happens over NVLink, which is completely invisible to SmartNICs on the InfiniBand ports. The SmartNICs would only see traffic that leaves the rack (e.g., checkpoint saves to remote storage, parameter server communication). This is a fundamental limitation that verification architectures must account for — you cannot rely solely on network monitoring to verify what's happening inside an NVLink domain."
/>

## Attestation Surfaces

Despite the NVLink blind spot, there are multiple **attestation surfaces** — points where verifiable information about cluster activity can be collected:

### 1. Hardware-Level Attestation
- **BMC (Baseboard Management Controller)** — Reports power draw, temperature, fan/pump speeds. A GPU under full training load has a distinctive power signature (~1,000-1,200W for Blackwell) that differs from idle (~50-100W) or inference workloads (~400-600W).
- **GPU telemetry (DCGM)** — Reports SM utilization, memory utilization, NVLink utilization. Requires cooperation from the operator (or a trusted agent on the node).
- **TPM/secure boot** — Can attest that the firmware and OS haven't been tampered with, establishing a chain of trust from hardware to software.

### 2. Network-Level Attestation
- **SmartNIC telemetry** — Observes InfiniBand traffic patterns, flow sizes, timing. Can characterize the communication signature of a training job.
- **Switch counters** — Aggregate traffic statistics per port. Available via InfiniBand UFM or similar management tools.
- **Network TAPs** — Physical devices that create a copy of traffic for analysis. Provide the highest fidelity but require physical installation.

### 3. Software-Level Attestation
- **Job scheduler logs** — Record what was scheduled, where, and when.
- **Training framework metrics** — Loss curves, gradient norms, throughput. A training job producing a smooth loss curve that matches expected behavior is strong evidence that real training is occurring.
- **Checkpoints** — Model weights saved periodically. A sequence of checkpoints showing gradual convergence is hard to fake.

### 4. Facility-Level Attestation
- **Utility power meters** — The most tamper-resistant data source. Utility-grade meters are owned by the power company, not the operator. They show total facility power draw at billing accuracy.
- **Water meters** — For facilities using evaporative cooling, water consumption correlates with heat rejection and thus compute load.
- **Physical access logs** — Badge reader data, security camera footage. Can verify who had physical access to the facility and when.

<ConceptCheck
  question="An international verification protocol requires proving that a training run used no more than N GPUs. Which combination of attestation surfaces provides the strongest evidence?"
  options={[
    {label: "(a) SmartNIC monitoring alone — it can see all traffic and count active GPUs"},
    {label: "(b) BMC power telemetry + network topology mapping + scheduler logs", correct: true},
    {label: "(c) Training loss curves — if the loss matches an N-GPU run, that proves it"},
    {label: "(d) Physical inspection of the facility to count installed GPUs"},
  ]}
  explanation="No single attestation surface is sufficient. BMC power telemetry can verify how many GPUs are drawing training-level power (distinctive power signature). Network topology mapping establishes which GPUs could communicate with the training job. Scheduler logs record the allocation. Cross-referencing these three sources is much stronger than any one alone. SmartNIC monitoring misses NVLink-domain activity. Loss curves can match many configurations. Physical GPU count tells you capacity but not usage."
/>

## The Workload Fingerprinting Problem

One of the most promising verification techniques is **workload fingerprinting** — identifying what type of computation is running based on observable signatures without needing to see the computation itself.

### Power Signatures

Different workloads produce distinctive power draw patterns:

| Workload | Power Pattern | Distinguishing Feature |
|----------|--------------|----------------------|
| Large model training | Sustained 85-95% of TDP | Very steady, hours-long plateaus with periodic dips (checkpointing) |
| Inference serving | 30-60% of TDP, variable | Bursty, correlates with request traffic |
| Idle | 5-10% of TDP | Low, flat baseline |
| Fine-tuning | 60-80% of TDP | Similar to training but shorter duration, smaller batch all-reduces |
| Cryptocurrency mining | 90-100% of TDP | Extremely steady, no communication pauses |

Power signatures are hard to fake because they're constrained by physics. You can't make a GPU draw training-level power without actually doing training-level computation (or wasting energy on purpose, which is expensive and detectable by the efficiency anomaly it creates).

### Network Communication Patterns

Training jobs have distinctive communication patterns that depend on the parallelism strategy:

- **Data parallel** training produces regular all-reduce bursts synchronized across all participating GPUs. The timing is predictable and the volume correlates with model size.
- **Pipeline parallel** training produces sequential data flows between pipeline stages, with a characteristic "bubble" pattern where some GPUs are idle while waiting for their turn.
- **Tensor parallel** training produces high-frequency, small all-reduce operations within tensor-parallel groups.

A verifier who can observe IB fabric traffic can determine which parallelism strategy is being used and estimate the model size from the gradient synchronization volume.

<ConceptCheck
  question="A verification system observes the InfiniBand fabric and sees regular, synchronized bursts of traffic every 2-3 seconds across 512 ports, with each burst carrying approximately 1 GB of data. What can you infer about the training run?"
  options={[
    {label: "(a) Nothing useful — this traffic pattern could be anything"},
    {label: "(b) The all-reduce pattern suggests data-parallel training with a model whose gradients total ~1 GB (roughly a 500M parameter model in FP16)", correct: true},
    {label: "(c) This is definitely inference traffic based on the burst pattern"},
    {label: "(d) The regular timing proves the system is running a benchmark, not real training"},
  ]}
  explanation="The regular, synchronized pattern across many ports is characteristic of data-parallel training with periodic gradient all-reduce. The 1 GB data volume per burst corresponds to the gradient size: a 500M parameter model in FP16 (2 bytes per parameter) produces ~1 GB of gradients per all-reduce step. The 2-3 second interval reflects the training step time. This kind of traffic analysis can estimate model size without ever seeing the model itself."
/>

## Verification Challenges

### The Cooperation Spectrum

Verification architectures exist on a spectrum from **fully cooperative** (operator provides full access and telemetry) to **adversarial** (operator actively tries to deceive the verifier).

Most practical verification scenarios are somewhere in between — **cooperative but untrusted**. The operator agrees to verification but might have incentives to misrepresent certain aspects of their operations (scale, timing, purpose of workloads).

### Specific Challenges

1. **NVLink opacity** — As discussed, intra-rack GPU communication is invisible to network monitoring. Verification must rely on indirect indicators (power signatures, BMC telemetry, software-level metrics) for activity within NVLink domains.

2. **Telemetry integrity** — If the verifier relies on software-reported metrics, how do you know the software hasn't been modified to report false data? Hardware-rooted trust (TPM, secure boot, signed firmware) is necessary to anchor the chain of trust. But even TPM attestation has limits — it verifies the boot chain, not the runtime behavior.

3. **Temporal verification** — Proving what happened in the past is harder than monitoring the present. Logs can be modified, and continuous monitoring requires persistent access. Cryptographic commitment schemes (hash chains, timestamped signatures) can provide some tamper-evidence, but they require the commitment infrastructure to have been in place before the period being verified.

4. **Multi-tenancy** — When a cluster runs multiple workloads, attributing specific resource usage to specific tenants requires fine-grained monitoring that may not be available. Cloud providers run many customers on shared infrastructure — can you verify that a specific training run used only N GPUs when other workloads are running on the same cluster?

5. **Distributed training across facilities** — Training runs can span multiple data centers connected by WAN links. Verifying that a run used only domestic compute when the operator has both domestic and international clusters requires monitoring the WAN interconnects, not just each facility independently.

<ConceptCheck
  question="An operator wants to secretly train a large model while reporting to a verifier that the cluster is idle. They have full control of the software stack. Which of the following is the hardest to fake?"
  options={[
    {label: "(a) DCGM-reported GPU utilization — just modify the DCGM output"},
    {label: "(b) Scheduler logs showing no jobs running — just delete the log entries"},
    {label: "(c) Utility power meter readings showing baseline facility power draw", correct: true},
    {label: "(d) Training loss curves — just don't report them"},
  ]}
  explanation="Utility power meters are owned by the power company, not the operator, and measure at billing accuracy. Training 10,000 GPUs draws ~12-15 MW of IT power. You cannot hide this power draw — the utility meter will show it regardless of what the operator's own monitoring systems report. This is why facility-level power monitoring is considered one of the strongest verification signals: it's hard to fake because the metering infrastructure is outside the operator's control."
/>

## Toward a Verification Architecture

A robust verification system for AI training infrastructure likely needs:

1. **Hardware-rooted trust** — Secure boot chains, TPM attestation, signed firmware. This ensures the software stack hasn't been tampered with.

2. **Multi-layer monitoring** — Combining BMC telemetry, network monitoring, and software metrics. No single layer is sufficient; cross-referencing catches inconsistencies.

3. **Continuous access** — Point-in-time audits are far weaker than continuous monitoring. A verifier needs persistent access to telemetry streams, not just periodic snapshots.

4. **Physical verification** — Some claims (facility location, hardware inventory) can only be verified with physical inspection.

5. **Cryptographic commitments** — Operators committing to telemetry data in real-time (e.g., signed, timestamped logs sent to a third party) makes after-the-fact tampering much harder.

6. **Independent metering** — Utility power data, water consumption records, and other facility metrics that are measured by parties other than the operator provide tamper-resistant baselines.

### The Cost of Verification

Verification isn't free. Every monitoring system adds:
- **Capital cost** — SmartNICs, TAPs, sensors, storage for telemetry data
- **Operational overhead** — Staff to manage monitoring infrastructure, analyze data, respond to anomalies
- **Performance impact** — Network TAPs add negligible latency, but software-based monitoring agents consume CPU/GPU cycles
- **Trust requirements** — Who operates the verification infrastructure? Who has access to the data? How is the verifier's integrity established?

Preliminary estimates suggest that a comprehensive verification system adds 1-3% to total cluster cost — much less than the 10-20% overhead of facility power and cooling. But the institutional and legal frameworks for who pays for verification and who has access are still being developed.

<Scenario
  title="Cross-border training verification"
  prompt="A government regulator asks you to verify that a training run was conducted entirely within national borders. The operator has a cluster spanning two data centers: one domestic, one in a neighboring country. They claim the training run used only the domestic cluster. The operator gives you access to SmartNIC telemetry, BMC logs, and scheduler records from the domestic facility."
  questions={[
    "What can you verify from the provided data?",
    "What can't you verify, and why?",
    "What additional evidence or access would you need to provide strong verification?",
    "Could the operator have used the foreign cluster for some portion of the training while making it appear entirely domestic?",
  ]}
  hints={<span>Key considerations: SmartNIC telemetry from the domestic facility can show whether traffic left the facility via WAN links (which would indicate cross-border communication). BMC logs can confirm which GPUs were active. Scheduler logs show the job allocation. However, you only have data from the domestic facility — you have no visibility into what was happening at the foreign facility. The operator could potentially have run a coordinated workload across both facilities using a carefully partitioned parallelism strategy that minimizes detectable cross-border traffic. Stronger verification would require: access to the WAN link monitoring (or physical TAP on the cross-border link), attestation from the foreign facility that its GPUs were idle during the relevant period, or a trusted third-party monitoring system with access to both facilities.</span>}
/>

---

*This is the final technical chapter. For a summary of unresolved problems and research questions, see [Open Questions](/docs/open-questions).*
