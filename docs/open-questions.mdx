---
sidebar_position: 7
title: Open Questions
slug: /open-questions
---

# Open Questions

These are problems we don't have good answers to yet. If you have insights, experience, or research relevant to any of these, we want to hear from you.

This page serves two purposes: it's an honest acknowledgment of what the field doesn't know, and it's a recruiting filter. If reading these questions makes you want to work on the answers, [get in touch](https://github.com/lucid-computing-labs/ai-infra-field-guide/issues).

---

## NVLink Opacity

**The core problem:** GPU-to-GPU communication within an NVLink domain (e.g., within an NVL72 rack of 72 GPUs) is invisible to network-based monitoring. SmartNICs, network TAPs, and switch telemetry only see traffic that crosses into the InfiniBand fabric. As NVLink domains grow larger (72 GPUs today, potentially more in future generations), the "blind spot" for network monitoring grows with them.

- Is there a viable approach to monitoring NVLink traffic without NVIDIA's cooperation at the firmware/hardware level?
- Can indirect indicators (power signatures, thermal patterns, memory access patterns via BMC) reliably characterize what's happening inside an NVLink domain?
- As rack-scale computing grows, does the verification architecture need to fundamentally change from network-centric to something else?

## Telemetry Integrity

**The core problem:** Most cluster telemetry is generated by software running on the same machines being monitored. If the operator controls the software stack, they can potentially modify telemetry to report false data.

- How far can hardware-rooted trust (TPM, secure boot, signed firmware) actually take us? What are the practical limits?
- Is there a role for confidential computing (TEEs) in creating trusted telemetry pipelines on untrusted hardware?
- Can cross-referencing multiple independent telemetry sources (BMC, SmartNIC, switch, facility power meters) detect inconsistencies reliably enough to catch tampering?

## Temporal Verification

**The core problem:** Verifying what's happening *right now* is significantly easier than verifying what happened in the past. Logs can be modified, and continuous monitoring requires persistent access that may not have been in place.

- What cryptographic commitment schemes could enable tamper-evident logging that operators can't retroactively modify?
- Is there a practical approach to "retroactive verification" — establishing facts about past compute usage from evidence available today?
- How do you handle the bootstrapping problem — verifying the period before the verification system itself was installed?

## Scale and Architecture Trends

**The core problem:** AI cluster architectures are changing rapidly. Verification approaches designed for today's infrastructure may not work for next-generation systems.

- How will CXL (Compute Express Link) memory pooling change the verification picture? When memory is disaggregated from compute, what does "observing a GPU's activity" even mean?
- What happens when training runs span multiple data centers connected by long-haul WAN? How do you verify the geographic distribution of compute?
- If custom silicon (ASICs from Google, Amazon, etc.) lacks the management interfaces available on NVIDIA GPUs, what alternative verification approaches exist?

## Economic and Incentive Questions

**The core problem:** Technical verification only matters if the institutional incentives support it. The economics of AI compute create specific incentive structures that verification designers need to understand.

- What is the cost overhead of a robust verification system? Is it 1% of cluster cost? 5%? 10%? At what point does the overhead become economically prohibitive?
- How do verification requirements interact with competitive dynamics? Does mandatory verification create advantages for large operators over small ones?
- Is there a viable business model for independent verification that doesn't depend on government mandates?

## What We Need Help With

If you have expertise in any of the following areas, we'd particularly value your input:

- **Hardware security** — TPM attestation at scale, secure boot chain integrity, hardware-rooted telemetry
- **Network forensics** — Traffic analysis techniques applicable to InfiniBand fabrics
- **Data center operations** — Practical experience with monitoring, alerting, and anomaly detection in large GPU clusters
- **Cryptographic protocols** — Commitment schemes, verifiable computation, zero-knowledge proofs applicable to compute verification
- **Policy and governance** — How verification requirements translate into practical compliance frameworks

File an issue on [GitHub](https://github.com/lucid-computing-labs/ai-infra-field-guide/issues) or submit a pull request. See [CONTRIBUTING.md](https://github.com/lucid-computing-labs/ai-infra-field-guide/blob/main/CONTRIBUTING.md) for guidelines.
